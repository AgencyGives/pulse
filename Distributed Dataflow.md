Flow-based programming is a powerful computational model that is becoming ever more relevant in today's highly concurrent multi-core, distributed systems. It is more powerful and as scalable as something like map-reduce. The premise is very simple: computation is described in terms of black-box processes with well defined inputs and outputs. Each process runs in its own thread, reads inputs from a set of input channels, does something and then writes output to a set of output channels. There is no globally shared data and therefore there are no race conditions possible. Thus, parallelism is inherent to this model and arbitrary computations can be represented with it. There have been attempts in the past to develop the idea as a fundamental programming paradigm (a book, probably easily findable around the internet, that describe the idea in detail is called "Flow-Based Programming" as I remember). More recently, some research on live systems where computation is immediately effective as the programmer edits the program use the same idea. Programming lower-level logic in a flow-based manner can seem cumbersome and inefficient. But for a coarse-grained type of processing, especially in a distributed setting, the paradigm is powerful and convenient.

The basic idea is that a processing node has a bunch of incoming streams of data and bunch of outgoing streams. It continuously receives data from its input and it writes to one or more of its outputs. A simple way to view this is that the incoming streams provide a continuous source of parameters for a function to perform a computation and produce a continuous stream of results. The advantages of the flow-based approach are that parallel asynchronous processing and memory management (disposal of intermediary results that are no longer needed) are obtained for free as inherent to the model. Also, processing based on a flow-based model will scale seamlessly on many CPUs and machines. The disadvantage is that the programming logic may seem unnatural at first and it may prove difficult to live without a global variable namespace. In any case, in our implementation, each processing node has access to an arbitrary global context object provided by the application - this opens the door to working with global, shared state if need be in isolated and carefully controlled cases such as outputting the final results of the computation to a database, or accessing some global, read-only parameters.

The dataflow HyperGraphDB application component offers an implementation of this concept that is distributed across machines. The flow network of processes and channels can be thought of as a directed hypergraph, where each channel is a directed hyperedge connecting all processes writing to it and all processes reading from it. In the distributed version, it is in fact represented as such.

The core, in-process part of the framework, however, is **independent** of HyperGraphDB and can be used standalone. The distributed version uses the HyperGraphDB peer-to-peer framework for communication and also persists the topology in the local database instance of each peer. The intended use is data processing with HyperGraphDB as ultimate storage, but that doesn't have to be the case - processing results can be stored in files, RDBMs or whatever. It is in theory possible not to persist anything and dispense with having HGDB database instances altogether.